---
title: "P8105 Homework 3"
author: Rebekah Hughes
output: github_document
---


```{r setup, include=FALSE}
library(tidyverse)
library(p8105.datasets)
library(patchwork)

knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_color_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

## Problem 1

The following code loads the `instacart` dataset for this problem.

```{r}
data("instacart")
```

The `instacart` dataset includes data regarding items ordered from a sample of instacart users in 2017. The observations are on the level of item ordered by the various users. There are `r ncol(instacart)` variables and `r nrow(instacart)` observations in the dataset. The data is a data frame with a mixture of character and numeric variables. The variables included contain information on the products ordered, the user, the time of day and week that the products were ordered, the aisle and the department the products were ordered from. Some noteworthy variables are the day of the week, the product, the aisle, and the time of day the products were ordered.


The following code chunk determines the number of aisles and aisles with the most items ordered from.

```{r}
instacart %>%
  count(aisle) %>% 
  arrange(desc(n))
```

Using the above code chunk, it can be determined there are 134 aisles and the aisles with the most items ordered from them are fresh vegetables, fresh fruits, and packaged vegetables and fruits.


The following code chunk makes a plot showing number of items ordered on each aisle.

```{r}
instacart %>%
  count(aisle) %>% 
  filter(n > 10000) %>% 
  mutate(
    aisle = factor(aisle),
    aisle = fct_reorder(aisle, n)
  ) %>% 
  ggplot(aes(x = aisle, y = n)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))
```

The plot starts out with the items ordered the least, such as butter and oils and vinegars and progresses to the items ordered the most, as noted from the previous code chunk.


The following code shows the three most popular items ordered from the baking ingredients, dog food care, and packaged vegetables aisles.

```{r}
instacart %>% 
  filter(aisle %in% c("baking ingredients", "dog food care", "packaged vegetables fruits")) %>% 
  group_by(aisle) %>% 
  count(product_name) %>% 
  mutate(rank = min_rank(desc(n))) %>% 
  filter(rank < 4) %>% 
  arrange(aisle, rank) %>% 
  knitr::kable()
```

The results in the table show that light brown sugar, snack sticks chicken and rice recipe dog treats, and organic baby spinach were the top selling items of each category.


The following code chunk creates a table showing the average time of day that Pink Lady Apples and Coffee Ice Cream were ordered.

```{r, message=FALSE}
instacart %>% 
  filter(product_name %in% c("Pink Lady Apples", "Coffee Ice Cream")) %>% 
  group_by(product_name, order_dow) %>% 
  summarize(mean_hour = mean(order_hour_of_day)) %>% 
  pivot_wider(
    names_from = order_dow,
    values_from = mean_hour
  ) %>% 
  knitr::kable()
```

According to the above table, it appears that Pink Lady Apples are ordered earlier in the day than Coffee Ice Cream for most days of the week.


## Problem 2

The following code load, tidies and wrangles the accelerometer dataset.

```{r, message=FALSE}
accel_df =
  read_csv("./data/accel_data.csv") %>% 
  janitor::clean_names() %>% 
  mutate(weekend_or_weekday = if_else(day %in% c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday"), "weekday", "weekend"),
         day = factor(day),
         day = fct_relevel(day, c("Monday", "Tuesday", "Wednesday", "Thursday", "Friday", "Saturday", "Sunday"))) %>% 
  pivot_longer(
  activity_1:activity_1440,
  names_to = "minute",
  names_prefix = "activity_",
  values_to = "activity"
) %>% 
  mutate(minute = as.numeric(minute))
```

The accelerometer dataset contains data regarding the amount of activity for one person over a 24 hour period for 5 weeks. There are `r ncol(accel_df)` variables and `r nrow(accel_df)` observations in the dataset. Key variables to note are the activity and minute variables, the day variable, the week variable, and the weekday versus weekend variable.


The next code chunk creates and displays a table with the total activity across a day.

```{r, message=FALSE}
accel_df %>% 
  group_by(day, day_id) %>% 
  summarize(total_activity = sum(activity)) %>% 
  pivot_wider(
    names_from = day,
    values_from = total_activity
  ) %>%
  knitr::kable()
```

Using the above table, it can de determined that there was more activity during the first weeks of the recording and there was less activity on Saturdays in general compared to other days of the week.


The following code creates a plot showing the activity across a 24 hour period for each day.

```{r, message=FALSE}
accel_df %>%
  ggplot(aes(x = minute, y = activity, color = day)) +
  geom_smooth(se = FALSE)
```

Based on the plot, it can be determined that the person is overall more active during the week compared to the weekend. There are also certain times of day that the person is more active including around minutes 500 and 1250. There is a big jump on Sunday around minute 650 and a jump on Friday around minute 1250. There is minimal activity at night on all days.


## Problem 3

The following code loads the `ny_noaa` dataset to be used for this problem.

```{r}
data("ny_noaa")
```

This dataset is adapted from an NOAA dataset regarding weather records from weather stations throughout the world over time. The observations are daily records of metrics including maximum and minimum temperatures, snowfall and depth, and precipitation at the various weather stations. There are `r ncol(ny_noaa)` variables and `r nrow(ny_noaa)` observations in the dataset. Some key variables are id as the station identifier, date, tmin and tmax for the temperatures, and precipitation and snowfall amounts. There are `r sum(is.na(ny_noaa))` missing values in the dataset, which could pose some issues.


The following code cleans the `ny_noaa` dataset and computes the number of most commonly observed values of snowfall.

```{r}
weather_df =
  ny_noaa%>% 
  mutate(date = as.character(date)) %>% 
  separate(date, c("year", "month", "day")) %>% 
  mutate(prcp = prcp/10,
         tmax = as.numeric(tmax), 
         tmin = as.numeric(tmin), 
         tmax = tmax/10,
         tmin = tmin/10,
         month = as.numeric(month)
)

weather_df %>% 
  count(snow) %>% 
  arrange(desc(n))
```

Given the above code, it can be determined that the most commonly observed amounts of snowfall are 0 mm and missing values. This may be because the weather stations sampled may only normally get 0 inches of snow each year and could also be because of the changing seasons during the year. Also, the missing values could be because there are already so many missing values throughout the dataset, as noted at the beginning of problem 3.


The following code chunk creates a plot showing the average maximum temperature in January and July in each station across years, with the month variable being converted to month name instead of number.

```{r, message=FALSE, warning=FALSE}
month_df =
  tibble(
    month = 1:12,
    month_name = month.name
  )

weather_plot =
  left_join(weather_df, month_df, by = "month") %>% 
  group_by(id, year, month_name) %>%
  summarise(mean_tmax = mean(tmax)) %>% 
  filter(month_name %in% c("January", "July")) %>% 
  ggplot(aes(x = year, y = mean_tmax)) +
  geom_point() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1)) +
  facet_grid(. ~ month_name)
```

Each point in the above plot represents a weather station. Most of the points in the above plot are clustered around each other for each year. However, there are some outliers in each of the month plots. In the January plot, there is an outlier on the lower end of the average maximum temperature in 1982. In July, there is a far outlier in 1988 and some closer outliers in 1984, 2004 and 2007.


The following code chunk creates a two-panel plot showing the maximum and minimum temperatures for the dataset and the distribution of snowfall values between, but not including, 0 and 100 mm seperately by year.

```{r, warning=FALSE}
tmax_tmin_plot =
  weather_df %>% 
  ggplot(aes(x = tmax, y = tmin)) +
  geom_bin2d() +
  theme(legend.position = "right")
  
snow_plot =
  weather_df %>% 
  group_by(year) %>% 
  filter(between(snow, 1, 99)) %>% 
  ggplot(aes(x = year, y = snow)) +
  geom_boxplot() +
  theme(axis.text.x = element_text(angle = 90, vjust = 0.5, hjust = 1))

tmax_tmin_plot + snow_plot
```

Given the above code, it is shown that as the maximum temperature increased, the minimum temperatures increased as well, clustering between 0 and 25 degrees Celsius often. There are also similar distributions of snowfall over the years, as shown by the box plots, but some of the years are slightly off and include outliers, such as 1998, 2006 and 2010.